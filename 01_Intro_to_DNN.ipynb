{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start of google colab specific stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import files  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# files.upload() # (to upload kaggle.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !mkdir ~/.kaggle # (makes hiddn folder in root directory of linux instance alloted in colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !mv {path of kaggle.json} ~/.kaggle # (moving kaggle.json to that hidden folder in linux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !chmod 600 ~/.kaggle/kaggle.json # (updating the permission to read, write only and not executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !kaggle datasets download -d sohier/calcofi # (to download dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !unzip {path of downloaded zip}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "End of google colab specific stuff <hr>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/jh/h8sdq7wd2gg3pb1y528l75m80000gn/T/ipykernel_2849/1660720635.py:1: DtypeWarning: Columns (47,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(\"bottle.csv\")\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"bottle.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cst_Cnt</th>\n",
              "      <th>Btl_Cnt</th>\n",
              "      <th>Sta_ID</th>\n",
              "      <th>Depth_ID</th>\n",
              "      <th>Depthm</th>\n",
              "      <th>T_degC</th>\n",
              "      <th>Salnty</th>\n",
              "      <th>O2ml_L</th>\n",
              "      <th>STheta</th>\n",
              "      <th>O2Sat</th>\n",
              "      <th>...</th>\n",
              "      <th>R_PHAEO</th>\n",
              "      <th>R_PRES</th>\n",
              "      <th>R_SAMP</th>\n",
              "      <th>DIC1</th>\n",
              "      <th>DIC2</th>\n",
              "      <th>TA1</th>\n",
              "      <th>TA2</th>\n",
              "      <th>pH2</th>\n",
              "      <th>pH1</th>\n",
              "      <th>DIC Quality Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>054.0 056.0</td>\n",
              "      <td>19-4903CR-HY-060-0930-05400560-0000A-3</td>\n",
              "      <td>0</td>\n",
              "      <td>10.50</td>\n",
              "      <td>33.440</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25.649</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>054.0 056.0</td>\n",
              "      <td>19-4903CR-HY-060-0930-05400560-0008A-3</td>\n",
              "      <td>8</td>\n",
              "      <td>10.46</td>\n",
              "      <td>33.440</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25.656</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>054.0 056.0</td>\n",
              "      <td>19-4903CR-HY-060-0930-05400560-0010A-7</td>\n",
              "      <td>10</td>\n",
              "      <td>10.46</td>\n",
              "      <td>33.437</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25.654</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>054.0 056.0</td>\n",
              "      <td>19-4903CR-HY-060-0930-05400560-0019A-3</td>\n",
              "      <td>19</td>\n",
              "      <td>10.45</td>\n",
              "      <td>33.420</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25.643</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>054.0 056.0</td>\n",
              "      <td>19-4903CR-HY-060-0930-05400560-0020A-7</td>\n",
              "      <td>20</td>\n",
              "      <td>10.45</td>\n",
              "      <td>33.421</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25.643</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 74 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cst_Cnt  Btl_Cnt       Sta_ID                                Depth_ID  \\\n",
              "0        1        1  054.0 056.0  19-4903CR-HY-060-0930-05400560-0000A-3   \n",
              "1        1        2  054.0 056.0  19-4903CR-HY-060-0930-05400560-0008A-3   \n",
              "2        1        3  054.0 056.0  19-4903CR-HY-060-0930-05400560-0010A-7   \n",
              "3        1        4  054.0 056.0  19-4903CR-HY-060-0930-05400560-0019A-3   \n",
              "4        1        5  054.0 056.0  19-4903CR-HY-060-0930-05400560-0020A-7   \n",
              "\n",
              "   Depthm  T_degC  Salnty  O2ml_L  STheta  O2Sat  ...  R_PHAEO  R_PRES  \\\n",
              "0       0   10.50  33.440     NaN  25.649    NaN  ...      NaN       0   \n",
              "1       8   10.46  33.440     NaN  25.656    NaN  ...      NaN       8   \n",
              "2      10   10.46  33.437     NaN  25.654    NaN  ...      NaN      10   \n",
              "3      19   10.45  33.420     NaN  25.643    NaN  ...      NaN      19   \n",
              "4      20   10.45  33.421     NaN  25.643    NaN  ...      NaN      20   \n",
              "\n",
              "   R_SAMP  DIC1  DIC2  TA1  TA2  pH2  pH1  DIC Quality Comment  \n",
              "0     NaN   NaN   NaN  NaN  NaN  NaN  NaN                  NaN  \n",
              "1     NaN   NaN   NaN  NaN  NaN  NaN  NaN                  NaN  \n",
              "2     NaN   NaN   NaN  NaN  NaN  NaN  NaN                  NaN  \n",
              "3     NaN   NaN   NaN  NaN  NaN  NaN  NaN                  NaN  \n",
              "4     NaN   NaN   NaN  NaN  NaN  NaN  NaN                  NaN  \n",
              "\n",
              "[5 rows x 74 columns]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Problem statement: Is there a relationship between water salinity & water temperature? Can you predict the water temperature based on salinity?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>Observations:\n",
        "- output (temperature) has range (-inf,+inf)\n",
        "- We keep those features in our dataset which are highly correlated (pearson correlation) to salinity. We will drop the rest features\n",
        "- $\\hat{y}$ : T_degC\n",
        "- 4 features (counts and IDS) given in starting are useless too.\n",
        "- There are lot of missing values also. Some columns (features) seems to have lot of missing values which makes them useless. Drop them"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr> Start of Preprocessing task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data = data.iloc[:,4:] # dropped first 4 columns (counts abd IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Depthm                      0\n",
              "T_degC                  10963\n",
              "Salnty                  47354\n",
              "O2ml_L                 168662\n",
              "STheta                  52689\n",
              "                        ...  \n",
              "TA1                    862779\n",
              "TA2                    864629\n",
              "pH2                    864853\n",
              "pH1                    864779\n",
              "DIC Quality Comment    864808\n",
              "Length: 70, dtype: int64"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reduced_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop those columns which does not have atleast 80% non NA values\n",
        "# or we can say columns with more than 20% of missing values are dropped\n",
        "reduced_data_copy = reduced_data.dropna(thresh=(0.8*reduced_data.shape[0]),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(864863, 22)"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reduced_data_copy.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will drop those rows in which output feature (T_degC) values are missing <br>\n",
        "\n",
        "Using boolean mask, we will be able to determine all the row indices where the values of column ```T_degC``` are missing. you will get ```np.array()``` of 10963 row indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop\n",
        "reduced_data_copy = reduced_data_copy[~reduced_data_copy['T_degC'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Depthm             0\n",
              "T_degC             0\n",
              "Salnty         39653\n",
              "O2ml_L        162613\n",
              "STheta         41726\n",
              "RecInd             0\n",
              "T_prec             0\n",
              "S_prec         39653\n",
              "NH3q           56564\n",
              "C14A1q         14641\n",
              "C14A2q         14643\n",
              "DarkAq         21754\n",
              "MeanAq         21755\n",
              "R_Depth            0\n",
              "R_TEMP             0\n",
              "R_POTEMP       35084\n",
              "R_SALINITY     39653\n",
              "R_SIGMA        41893\n",
              "R_SVA          41808\n",
              "R_DYNHT        39264\n",
              "R_O2          162613\n",
              "R_PRES             0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reduced_data_copy.isna().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "imputation of missing values using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform the data\n",
        "df = imputer.fit_transform(reduced_data_copy)\n",
        "\n",
        "# Convert the NumPy array back to a DataFrame\n",
        "df = pd.DataFrame(df, columns=reduced_data_copy.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Depthm        0\n",
              "T_degC        0\n",
              "Salnty        0\n",
              "O2ml_L        0\n",
              "STheta        0\n",
              "RecInd        0\n",
              "T_prec        0\n",
              "S_prec        0\n",
              "NH3q          0\n",
              "C14A1q        0\n",
              "C14A2q        0\n",
              "DarkAq        0\n",
              "MeanAq        0\n",
              "R_Depth       0\n",
              "R_TEMP        0\n",
              "R_POTEMP      0\n",
              "R_SALINITY    0\n",
              "R_SIGMA       0\n",
              "R_SVA         0\n",
              "R_DYNHT       0\n",
              "R_O2          0\n",
              "R_PRES        0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(853900, 22)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y_train = np.array(df[\"T_degC\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = np.array(df.drop(labels=['T_degC'],axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(853900, 21)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "End of preprocessing task <hr>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xG8Cw13XsTUU"
      },
      "source": [
        "Creating Shallow network with two hidden layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "kYO6oHVynbJM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input # for input layer\n",
        "from keras.layers import Dense # for hidden layer + output layer\n",
        "from keras.models import Sequential # SISO , no skipping of any layer for data flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "hDMmh_rHtD-W"
      },
      "outputs": [],
      "source": [
        "def create_dnn(): # densely connected network\n",
        "  dnn = Sequential() # made an object of Sequential class, data get processed sequentially\n",
        "  dnn.add(Input(shape=(21,))) # input layer added\n",
        "  dnn.add(Dense(units=21,activation=\"relu\")) # 4 neurons hidden layer\n",
        "  dnn.add(Dense(units=21,activation=\"relu\")) # another hidden layer\n",
        "  dnn.add(Dense(units=1)) # output layer # default is \"Linear Activation Function\"\n",
        "  return dnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "oq5ezwgLwwtZ"
      },
      "outputs": [],
      "source": [
        "dnn = create_dnn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxL1SdHkq7XA",
        "outputId": "678287f5-14c1-4f26-e60b-ef24b0e86ed9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x1687e43a0>"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dnn # our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlA8EYT_q8I0",
        "outputId": "4d2c0f6f-5547-4e52-80f0-1027bc469262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 21)                462       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 21)                462       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 22        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 946 (3.70 KB)\n",
            "Trainable params: 946 (3.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "dnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09s52D5M2go_",
        "outputId": "ef659493-6af5-4c1c-d69c-f190805d7c0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<keras.src.layers.core.dense.Dense at 0x2ad8e0d90>,\n",
              " <keras.src.layers.core.dense.Dense at 0x1687e4130>,\n",
              " <keras.src.layers.core.dense.Dense at 0x2cfa055b0>]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dnn.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "tQej58jO2pC_"
      },
      "outputs": [],
      "source": [
        "# dnn.layers[0].trainable = False # attribute modified, made the parameters of first layer non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "7DNJTf5d5wap"
      },
      "outputs": [],
      "source": [
        "dnn.compile(optimizer=\"sgd\", loss=\"mse\",metrics=[tf.keras.metrics.RootMeanSquaredError()]) # setting the environment to train our model.\n",
        "# loss function, training algorithm (optimizer), error metric\n",
        "# default optimizer is rmsprop"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 3 ways of passing data to our NN model to fit it:\n",
        "1. Using custom training data generator\n",
        "2. Using Keras training data generator\n",
        "3. Directly passing the data. (X_train, Y_train) <br>\n",
        "\n",
        "Direclty passing the data puts the all data in RAM at once. RAM will be fully occupied at once in case of large dataset. That's why generator is used which gives data to NN in batches to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "   1/3416 [..............................] - ETA: 16:02 - loss: 2257.5735 - root_mean_squared_error: 47.5139"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-03 21:26:31.512586: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3416/3416 [==============================] - ETA: 0s - loss: 500110991556608.0000 - root_mean_squared_error: 22363162.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-03 21:26:48.420412: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3416/3416 [==============================] - 19s 6ms/step - loss: 500110991556608.0000 - root_mean_squared_error: 22363162.0000 - val_loss: 14.3046 - val_root_mean_squared_error: 3.7821\n",
            "Epoch 2/25\n",
            "3416/3416 [==============================] - 18s 5ms/step - loss: 19.0553 - root_mean_squared_error: 4.3652 - val_loss: 14.3147 - val_root_mean_squared_error: 3.7835\n",
            "Epoch 3/25\n",
            "3416/3416 [==============================] - 18s 5ms/step - loss: 19.0553 - root_mean_squared_error: 4.3652 - val_loss: 14.3410 - val_root_mean_squared_error: 3.7870\n",
            "Epoch 4/25\n",
            "1840/3416 [===============>..............] - ETA: 7s - loss: 19.0786 - root_mean_squared_error: 4.3679"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dnn\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mX_train, y\u001b[39m=\u001b[39;49mY_train,\n\u001b[1;32m      2\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, \n\u001b[1;32m      3\u001b[0m         validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m\n\u001b[1;32m      5\u001b[0m         ) \u001b[39m# method 3 of fitting\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dnn.fit(x=X_train, y=Y_train,\n",
        "        batch_size=200, \n",
        "        validation_split=0.2,\n",
        "        epochs=25\n",
        "        ) # method 3 of fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mini_batch_size=80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "683120.0"
            ]
          },
          "execution_count": 502,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_size = df.shape[0]-df.shape[0]*0.20\n",
        "validation_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_data_generator():\n",
        "    while True: # to repeat the yield\n",
        "        for i in range(int(df.shape[0]-df.shape[0]*0.20)//mini_batch_size): # 20% data kept for validation\n",
        "            yield X_train[i*mini_batch_size:(i+1)*mini_batch_size,:], Y_train[i*mini_batch_size:(i+1)*mini_batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen = training_data_generator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<generator object training_data_generator at 0x2b7bbe040>"
            ]
          },
          "execution_count": 505,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datagen  # (output of lazy function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(80,)"
            ]
          },
          "execution_count": 506,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datagen.__next__()[1].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## .fit parameters guide:\n",
        "Value of **batch size** should divide the size of training data. <br> \n",
        "When the number of training examples (𝑁) is not a multiple of the batch size (𝐵), Keras creates an additional batch for the remaining data. In other words, you end up having  (𝑁 𝑑𝑖𝑣 𝐵)+1 batches, and the last batch has  (𝑁 𝑚𝑜𝑑 𝐵) training examples. <br> <br>\n",
        "**verbosity**: while training we will be shown some activities which indicattes that our model is getting trained <br> <br>\n",
        "**callbacks**: functions which supervises the training. Take some actions when see some chaos (like overfitting) during training. These callbacks can be custom or provided by in built by Keras <br> <br>\n",
        "**validation_data**: pass tuple of features amd output directly or a generator of those tuples <br> <br>\n",
        "**validation_split**: This is used when you passed whole data for training. This value specify what percentage of data to be taken for validation <br> <br>\n",
        "**shuffle**: shuffles the order of training data points after each epoch <br> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_data = (X_train[int(df.shape[0]-df.shape[0]*0.20):], Y_train[int(df.shape[0]-df.shape[0]*0.20):])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-26 23:10:32.480065: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[525], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dnn\u001b[39m.\u001b[39;49mfit(training_data_generator(),\n\u001b[1;32m      2\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m         steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(df\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\u001b[39m-\u001b[39;49mdf\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\u001b[39m*\u001b[39;49m\u001b[39m0.20\u001b[39;49m)\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49mmini_batch_size,\n\u001b[1;32m      4\u001b[0m         validation_data\u001b[39m=\u001b[39;49mvalidation_data\n\u001b[1;32m      5\u001b[0m         ) \n\u001b[1;32m      6\u001b[0m \u001b[39m# method 1\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/Documents/ML_Internship/mlintern/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:965\u001b[0m, in \u001b[0;36mTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__tf_tensor__\u001b[39m(\n\u001b[1;32m    962\u001b[0m     \u001b[39mself\u001b[39m, dtype: Optional[dtypes\u001b[39m.\u001b[39mDType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTensor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    964\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m dtype\u001b[39m.\u001b[39mis_compatible_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m--> 965\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    966\u001b[0m         _add_error_prefix(\n\u001b[1;32m    967\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensor conversion requested dtype \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    968\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor Tensor with dtype \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    969\u001b[0m             name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    970\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
          ]
        }
      ],
      "source": [
        "dnn.fit(training_data_generator(),\n",
        "        epochs=15,\n",
        "        steps_per_epoch=int(df.shape[0]-df.shape[0]*0.20)//mini_batch_size,\n",
        "        validation_data=validation_data\n",
        "        ) \n",
        "# method 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
